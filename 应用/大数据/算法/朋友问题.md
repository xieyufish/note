## Hadoop+MapReducer：共同朋友问题代码实现

在工作或者在我们平时Hadoop方面的面试中，我们经常会遇到这样一个问题：给我们一个好友关系列表，找出列表中两人之间的共同好友。例如，我们有如下数据：

```
A,B,C,D,E,F
B,A,C,D,E
C,A,B,E
D,A,B,E
E,A,B,C,D
F,A
```

其中，第一列表示用户自己，其他列是用户的好友，现在要从这个列表中找出AB，AC，BC，......，两两之间的共同好友。比如如上列表数据中，AB之间的共同好友有：CDE，BC之间的共同好友有：AE。

当我遇到这个问题的时候，总是从用户本身这个角度去想，我是这么思考的，第一列都是用户，那我们只要求每两个用户之间的好友列表是否有交集就肯定可以找出两个用户之间的共同好友，这种想法没错，理论上也确实可以找出用户之间的共同好友。但是，首先如果这样子计算，毫无疑问这完全不适合MapReduce的编程模型；第二点如果用户量很大的话，这个的计算量是非常庞大的（如果用户有100000，两两组合就得有100000！中可能，这就意味着要进行100000！次集合的比对，100000！这个数据有多大大家可以去算一算，而且这里还只算是100000用户量）。所以，显然这种想法是完全不可取的。

我这榆木脑袋，确实是没有找到一种好的方式。最后在网上查找到两种不同的实现方法，特此在这里做个总结记录。

### 第一种方式：求好友列表的交集

这一种方式的想法思路跟我的思路类似，也是通过求好友列表的交集来求的共同好友，但是却做了大大的简化。我的那种想法是要用户跟系统中的其他所有用户去求，而这里他的思路是只从用户和他的好友列表之中的用户两两组合去求。还是以上面的数据做为例子进行说明，比如上面第一行数据：A,B,C,D,E,F，那么AB直接的共同好友很显然只可能出现在CDEF这个集合中（由A的好友列表限制），同理AC之间的共同好友也只可能出现在BCEF这个集合中；依次类推，在第二行数据中，BA直接的共同好友只可能出现在CDE这个集合中；以此类推下去，要求AB之间的共同好友，我们只要找出AB和BA对应的两个好友集合的交集就可以找出AB直接的共同好友。恰好这样的一种计算模型也非常的适合去MapReduce的编程框架。下面是代码实现：

**Mapper类实现**

```java
package com.shell.dataalgorithms.mapreduce.chap08;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class CommonFriendsMapper extends Mapper<LongWritable, Text, Text, Text> {
	
	private static final Text REDUCER_KEY = new Text();
	private static final Text REDUCER_VALUE = new Text();
	
	String getFriends(String[] tokens) {
		if (tokens.length == 2) {
			return "";
		}
		
		StringBuilder builder = new StringBuilder();
		for (int i = 1; i < tokens.length; i++) {
			builder.append(tokens[i]);
			if (i < tokens.length - 1) {
				builder.append(",");
			}
		}
		
		return builder.toString();
	}
	
    // 创建key, A,B和B,A要统一成A,B
	String buildSortedKey(String person, String friend) {
		if (person.compareTo(friend) < 0) {
			return person + "," + friend;
		} else {
			return friend + "," + person;
		}
	}
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context)
			throws IOException, InterruptedException {
		
		String[] tokens = value.toString().split(",");
		
		String friends =  getFriends(tokens);
		REDUCER_VALUE.set(friends);
		
		String person = tokens[0];
		
		for (int i = 1; i < tokens.length; i++) {
			String friend = tokens[i];
			String reduceKeyAsString = buildSortedKey(person, friend);
			REDUCER_KEY.set(reduceKeyAsString);
			context.write(REDUCER_KEY, REDUCER_VALUE);
		}
	}
}
```

**Reducer类实现**
```java
package com.shell.dataalgorithms.mapreduce.chap08;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

// 注意Reducer中共同好友交集的求解方式
public class CommonFriendsReducer extends Reducer<Text, Text, Text, Text> {
	// 迭代次数可以计算出每个键对应的集合数有多少, 比如AB对应了两个集合[CDEF]和[CDE]
    // 那么只要元素出现的次数等于集合的个数也就意味着这个元素在每个集合中都出现了
    // 说明这个元素是共同拥有的, 即是共同好友
	@Override
	protected void reduce(Text key, Iterable<Text> values, Reducer<Text, Text, Text, Text>.Context context)
			throws IOException, InterruptedException {
		
		Map<String, Integer> map = new HashMap<String, Integer>();
		Iterator<Text> iterator = values.iterator();
		int numOfValues = 0;
		
		while (iterator.hasNext()) {
			String friends = iterator.next().toString();
			if (friends.equals("")) {
				context.write(key, new Text("[]"));
				return;
			}
			addFriends(map, friends);
			numOfValues++;
		}
		
		List<String> commonFriends = new ArrayList<>();
		for (Map.Entry<String, Integer> entry : map.entrySet()) {
            // 这里要过滤掉numOfValues等于1的情况, 等于1意味着只存在AB这边的一个集合[DBEF]
            // 而不存在[CDE]这个集合, 显然这这种情况下AB之间是没有共同好友的
			if (numOfValues > 1 && entry.getValue() == numOfValues) {  
				commonFriends.add(entry.getKey());
			}
		}
		
		context.write(key, new Text(commonFriends.toString()));
		
	}
	
	void addFriends(Map<String, Integer> map, String friendsList) {
		String[] friends = friendsList.split(",");
		for (String friend : friends) {
			Integer count = map.get(friend);
			if (count == null) {
				map.put(friend, 1);
			} else {
				map.put(friend, ++count);
			}
		}
	}

}
```

**Main类**
```java
package com.shell.dataalgorithms.mapreduce.chap08;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class CommonFriendsDriver extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		
		Job job = Job.getInstance();
		job.setJarByClass(getClass());
		job.setJobName(getClass().getSimpleName());
		
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.setMapperClass(CommonFriendsMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		job.setReducerClass(CommonFriendsReducer.class);
		
		job.waitForCompletion(true);
		
		return 0;
	}
	
	public static void main(String[] args) throws Exception {
		if (args.length != 2) {
            throw new IllegalArgumentException("usage: Argument 1: input dir, Argument 2: output dir");
        }
		
		ToolRunner.run(new CommonFriendsDriver(), args);
	}

}
```
用这一种方式，基本上可以找出两人之间的共同好友，但是有一些漏洞。比如：考虑如下一种数据情况。

```
A,B,C,E
B,E,F
C,A,B
E,B,A
F,B
```

当用上面的实现来处理这几条数据的时候，你会发现输出结果中AB之间的共同好友是为空的，但其实这里AB之间的共同好友是有CE的，为什么会出现这样一种情况呢？原因就在于好友关系的双向性，从这几行数据我们可以发现B在A的好友列表中，但是A确不在B的好友列表中，从而造成在Mapper端构建Key-Value对时数据的不完整性。

### 第二种方式：从好友列表的角度着手

从上面的思路来看，其实我们都是从用户角度去着手思考。其实我们在考虑这个问题的时候，忽略了一个很简单的规则。被这个问题的迷惑了，因为是查找两个用户之间的共同好友，所以我们解决问题的角度就总是从用户本身去看的，但你其实发现没有，共同好友：当AB拥有共同好友C时，这种情况下无论如何AB这两个用户肯定是会出现在C的好友列表中的，所以如果我们从好友列表这个角度去看，问题就变得相当简单和容易理解了。既然A有好友BCD，那毫无疑问BC的共同好友里面肯定有A啊。所以现在问题就变成两两组合每一行数据中的好友列表，再进行Reducer就可以很简单的找出共同好友了。具体代码实现如下：

**Mapper类实现**

```java
package com.shell.dataalgorithms.mapreduce.chap08;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class SecondCommonFriendsMapper extends Mapper<LongWritable, Text, Text, Text> {
	
	private List<String> getFriends(String[] tokens) {
		List<String> friends = new ArrayList<>();
		for (int i = 1; i < tokens.length; i++) {
			friends.add(tokens[i]);
		}
		
		return friends;
	}
	
	private String createKey(String friend1, String friend2) {
		if (friend1.compareTo(friend2) < 0) {
			return friend1 + friend2;
		} else {
			return friend2 + friend1;
		}
	}
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context)
			throws IOException, InterruptedException {
		
		String[] tokens = value.toString().split(",");
		
		String person = tokens[0];
		List<String> friends = getFriends(tokens);
		
		for (int i = 0; i < friends.size(); i++) {
			for (int j = i + 1; j < friends.size(); j++) {
				context.write(new Text(createKey(friends.get(i), friends.get(j))), new Text(person));
			}
		}
	}

}
```

**Reducer类实现**

```java
package com.shell.dataalgorithms.mapreduce.chap08;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class SecondCommonFriendsReducer extends Reducer<Text, Text, Text, Text> {
	
	@Override
	protected void reduce(Text key, Iterable<Text> values, Reducer<Text, Text, Text, Text>.Context context)
			throws IOException, InterruptedException {
		
		List<String> commonFriends = new ArrayList<>();
		for (Text value : values) {
			commonFriends.add(value.toString());
		}
		
		context.write(key, new Text(commonFriends.toString()));
	}

}
```

**Main类**

```java
package com.shell.dataalgorithms.mapreduce.chap08;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class SecondCommonFriendsDriver extends Configured implements Tool {

	@Override
	public int run(String[] args) throws Exception {
		
		Job job = Job.getInstance();
		job.setJarByClass(this.getClass());
		job.setJobName(this.getClass().getSimpleName());
		
		job.setInputFormatClass(TextInputFormat.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		
		job.setMapperClass(SecondCommonFriendsMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		job.setReducerClass(SecondCommonFriendsReducer.class);
		
		job.setOutputFormatClass(TextOutputFormat.class);
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.waitForCompletion(true);
		return 0;
	}
	
	public static void main(String[] args) throws Exception {
		if (args.length != 2) {
            throw new IllegalArgumentException("usage: Argument 1: input dir, Argument 2: output dir");
        }
		
		ToolRunner.run(new SecondCommonFriendsDriver(), args);
	}

}
```

